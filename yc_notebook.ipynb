{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import argparse\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "def load_yoga_postures(posture_dir):\n",
    "    yoga_postures = []\n",
    "    yoga_posture_labels = []\n",
    "\n",
    "    for posture_name in os.listdir(posture_dir):\n",
    "        if posture_name.startswith(\".\"):\n",
    "            continue\n",
    "        posture_folder = os.path.join(posture_dir, posture_name)\n",
    "        for image_name in os.listdir(posture_folder):\n",
    "            if image_name.startswith(\".\"):\n",
    "                continue\n",
    "            image_path = os.path.join(posture_folder, image_name)\n",
    "            if image_name.endswith(\".jpg\") or image_name.endswith(\".png\"):\n",
    "                image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                image = cv2.resize(image, (64, 64))\n",
    "                yoga_postures.append(image)\n",
    "                yoga_posture_labels.append(posture_name)\n",
    "\n",
    "    return yoga_postures, yoga_posture_labels\n",
    "\n",
    "def train_svm_model(X_train, yoga_posture_labels):\n",
    "    svm = SVC(kernel=\"linear\", C=1.0, random_state=42)\n",
    "    svm.fit(X_train, yoga_posture_labels)\n",
    "    return svm\n",
    "\n",
    "def classify_posture(frame, pose, svm, threshold=0.75):\n",
    "        # Convert the frame to grayscale\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # Detect the pose landmarks using Mediapipe\n",
    "        results = pose.process(frame_gray)\n",
    "        if results.pose_landmarks is not None:\n",
    "            # Extract the pose landmarks and calculate the HOG features\n",
    "            landmarks = np.array([[lmk.x, lmk.y] for lmk in results.pose_landmarks.landmark])\n",
    "            landmarks = landmarks.flatten()\n",
    "            # Predict the posture using the SVM model\n",
    "            scores = svm.decision_function([landmarks])\n",
    "            if np.max(scores) >= threshold:\n",
    "                posture_pred = svm.predict([landmarks])[0]\n",
    "            else:\n",
    "                posture_pred = None\n",
    "        else:\n",
    "            posture_pred = None\n",
    "        return posture_pred\n",
    "\n",
    "def display_posture(frame, pose, posture_pred):\n",
    "    # Draw the pose landmarks and the predicted posture on the frame\n",
    "    mp_drawing.draw_landmarks(frame, pose.process(frame).pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    cv2.putText(frame, posture_pred, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def save_video(frame, out):\n",
    "    out.write(frame)\n",
    "\n",
    "def yoga_classifier(input_path, display=False, output_path=None, save_model_path=None):\n",
    "    # Load the yoga postures and their labels\n",
    "    posture_dir = \"/home/khaleb.dabakuyo@Digital-Grenoble.local/Documents/ACV/Panther_trainer2/assets/images/train\"\n",
    "    yoga_postures, yoga_posture_labels = load_yoga_postures(posture_dir)\n",
    "\n",
    "    if save_model_path is not None and os.path.exists(save_model_path):\n",
    "        # Load the saved SVM model\n",
    "        svm = joblib.load(save_model_path)\n",
    "    else:\n",
    "        # Extract features from the images using mediapipe\n",
    "        X_train = []\n",
    "        for posture in yoga_postures:\n",
    "            results = mp_pose.Pose().process(posture)\n",
    "            if results.pose_landmarks is not None:\n",
    "                landmarks = np.array([[lmk.x, lmk.y] for lmk in results.pose_landmarks.landmark])\n",
    "                landmarks = landmarks.flatten()\n",
    "                X_train.append(landmarks)\n",
    "        X_train = np.array(X_train)\n",
    "        # Train a SVM model on the extracted features\n",
    "        svm = train_svm_model(X_train, yoga_posture_labels)\n",
    "        if save_model_path is not None:\n",
    "            # Save the trained SVM model\n",
    "            joblib.dump(svm, save_model_path)\n",
    "\n",
    "    # Initialize Mediapipe pose detection\n",
    "    pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "    # Use the trained model to classify new images of the yoga postures\n",
    "    if input_path == \"camera\":\n",
    "        cap = cv2.VideoCapture(0)\n",
    "    else:\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "    if output_path is not None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, 20.0, (640, 480))\n",
    "    start_time = time.time()\n",
    "    posture_timer = 0\n",
    "    posture_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        posture_pred = classify_posture(frame, pose, svm)\n",
    "        if posture_pred == \"maintained\":\n",
    "            posture_timer = time.time() - start_time\n",
    "            if posture_timer >= 30:\n",
    "                # Flash the image\n",
    "                cv2.imshow('frame', frame)\n",
    "                cv2.waitKey(1000)\n",
    "                cv2.imshow('frame', np.zeros_like(frame))\n",
    "                cv2.waitKey(1000)\n",
    "                # Display progress line\n",
    "                posture_count += 1\n",
    "                progress = int((posture_timer / 30) * 100)\n",
    "                cv2.putText(frame, f\"Progress: {progress}%\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        else:\n",
    "            start_time = time.time()\n",
    "            posture_timer = 0\n",
    "        if display:\n",
    "            display = display_posture(frame, pose, posture_pred)\n",
    "            if not display:\n",
    "                break\n",
    "        if output_path is not None:\n",
    "            save_video(frame, out)\n",
    "    cap.release()\n",
    "    if output_path is not None:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n",
      "Premature end of JPEG file\n",
      "I0000 00:00:1700046578.734850   30009 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1700046578.795137   30872 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 525.125.06), renderer: NVIDIA GeForce GTX 1650/PCIe/SSE2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/khaleb.dabakuyo@Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m yoga_classifier(\u001b[39m'\u001b[39m\u001b[39mcamera\u001b[39m\u001b[39m'\u001b[39m, display\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, output_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, save_model_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrue\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/khaleb.dabakuyo@Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb Cell 2\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W1sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m X_train \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W1sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39mfor\u001b[39;00m posture \u001b[39min\u001b[39;00m yoga_postures:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W1sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     results \u001b[39m=\u001b[39m mp_pose\u001b[39m.\u001b[39mPose()\u001b[39m.\u001b[39mprocess(posture)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W1sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W1sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m         landmarks \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[lmk\u001b[39m.\u001b[39mx, lmk\u001b[39m.\u001b[39my] \u001b[39mfor\u001b[39;00m lmk \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks\u001b[39m.\u001b[39mlandmark])\n",
      "File \u001b[0;32m~/anaconda3/envs/acv/lib/python3.11/site-packages/mediapipe/python/solutions/pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, image: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NamedTuple:\n\u001b[1;32m    165\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mprocess(input_data\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m: image})\n\u001b[1;32m    186\u001b[0m   \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks:  \u001b[39m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[39mfor\u001b[39;00m landmark \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks\u001b[39m.\u001b[39mlandmark:  \u001b[39m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/acv/lib/python3.11/site-packages/mediapipe/python/solution_base.py:360\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    355\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSolutionBase can only process non-audio and non-proto-list data. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    356\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_stream_type_info[stream_name]\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    357\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtype is not supported yet.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    358\u001b[0m \u001b[39melif\u001b[39;00m (input_stream_type \u001b[39m==\u001b[39m PacketDataType\u001b[39m.\u001b[39mIMAGE_FRAME \u001b[39mor\u001b[39;00m\n\u001b[1;32m    359\u001b[0m       input_stream_type \u001b[39m==\u001b[39m PacketDataType\u001b[39m.\u001b[39mIMAGE):\n\u001b[0;32m--> 360\u001b[0m   \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m] \u001b[39m!=\u001b[39m RGB_CHANNELS:\n\u001b[1;32m    361\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInput image must contain three channel rgb data.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    362\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    363\u001b[0m       stream\u001b[39m=\u001b[39mstream_name,\n\u001b[1;32m    364\u001b[0m       packet\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    365\u001b[0m                                data)\u001b[39m.\u001b[39mat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simulated_timestamp))\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "yoga_classifier('camera', display=True, output_path=None, save_model_path='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/khaleb.dabakuyo@Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m num_to_class_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m0\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mdowndog\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m1\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mgoddess\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m2\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mplank\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m3\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtree\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m4\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mwarrior2\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m }\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m class_to_num_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdowndog\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mgoddess\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwarrior2\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m4\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[1;32m/home/khaleb.dabakuyo@Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m num_to_class_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m0\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mdowndog\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m1\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mgoddess\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m2\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mplank\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m3\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtree\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m4\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mwarrior2\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m }\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m class_to_num_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdowndog\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mgoddess\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwarrior2\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m4\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W2sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/acv/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/acv/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_to_class_dict = {\n",
    "    0: \"downdog\",\n",
    "    1: \"goddess\",\n",
    "    2: \"plank\",\n",
    "    3: \"tree\",\n",
    "    4: \"warrior2\",\n",
    "}\n",
    "\n",
    "class_to_num_dict = {\n",
    "    \"downdog\": 0,\n",
    "    \"goddess\": 1,\n",
    "    \"plank\": 2,\n",
    "    \"tree\": 3,\n",
    "    \"warrior2\": 4,\n",
    "}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from streamClassifier.utils import build_dataframe\n",
    "from streamClassifier.config import num_to_class_dict, class_to_num_dict\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import os\n",
    "from PoseClassification.bootstrap_copy import BootstrapHelper\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "\n",
    "\n",
    "class SvcClassifier:\n",
    "    def __init__(self, training_csv_dir: str, stream_csv_dir:str):\n",
    "        self.model = None\n",
    "        self._training_dataset = self._get_training_dataset(training_csv_dir)\n",
    "        self._stream_csv_dir = stream_csv_dir\n",
    "\n",
    "    def _get_training_dataset(self, training_csv_dir) -> pd.DataFrame:\n",
    "        df = build_dataframe(source_dir=training_csv_dir)\n",
    "        return df\n",
    "\n",
    "    def _prepare_X_y_set(self) -> tuple:\n",
    "        df = self._training_dataset\n",
    "        X = df.drop(\n",
    "            [\"filename\", \"class\", \"class_num\"], axis=1\n",
    "        )  # Assuming 'label' is the column with class names/numbers\n",
    "        y = df[\"class_num\"]\n",
    "        return X, y\n",
    "    \n",
    "    def get_training_data_infos(self) -> dict:\n",
    "        infos = {\n",
    "            \"number_of_samples\": len(self._training_dataset),\n",
    "            \"num_class_0\": len(self._training_dataset[self._training_dataset[\"class_num\"] == 0]),\n",
    "            \"num_class_1\": len(self._training_dataset[self._training_dataset[\"class_num\"] == 1]),\n",
    "            \"num_class_2\": len(self._training_dataset[self._training_dataset[\"class_num\"] == 2]),\n",
    "            \"num_class_3\": len(self._training_dataset[self._training_dataset[\"class_num\"] == 3]),\n",
    "            \"num_class_4\": len(self._training_dataset[self._training_dataset[\"class_num\"] == 4]),\n",
    "        }\n",
    "        return infos\n",
    "    \n",
    "    def get_precision_infos(self):\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(self._confusion_matrix)\n",
    "        print(\"Classification Report:\")\n",
    "        print(self._classification_report)\n",
    "    \n",
    "    def fit(self,  show_value: bool = True):\n",
    "        \"\"\"\n",
    "        Evaluates the model\n",
    "        \"\"\"\n",
    "        # Assuming X is your feature matrix and y is the target vector\n",
    "        X, y = self._prepare_X_y_set()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "         X, y, test_size=0.3, random_state=42\n",
    "        )\n",
    "\n",
    "        svc = SVC(kernel=\"linear\")  # You can change the kernel based on your data characteristics\n",
    "        svc.fit(X_train, y_train)\n",
    "        y_pred = svc.predict(X_test)\n",
    "\n",
    "        self._confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os, csv\n",
    "import sys\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from mediapipe.python.solutions import drawing_utils as mp_drawing\n",
    "from mediapipe.python.solutions import pose as mp_pose\n",
    "\n",
    "class PoseClassifier:\n",
    "    def __init__(self, stream_csv_dir: str, show_value: bool = True):\n",
    "        self._stream_csv_dir = stream_csv_dir\n",
    "        self._confusion_matrix = None\n",
    "        self._classification_report = None\n",
    "        self.model = None\n",
    "        self.show_value = show_value\n",
    "\n",
    "    def fit(self) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model\n",
    "        \"\"\"\n",
    "        df = build_dataframe(source_dir=self._stream_csv_dir)\n",
    "        X = df.drop(\n",
    "            [\"filename\", \"class\", \"class_num\"], axis=1\n",
    "        )\n",
    "        y = df[\"class_num\"]\n",
    "\n",
    "        svc = SVC(kernel=\"linear\", C=1, gamma=\"auto\")\n",
    "        svc.fit(X, y)\n",
    "\n",
    "        y_pred = svc.predict(X)\n",
    "        self._confusion_matrix = confusion_matrix(y, y_pred)\n",
    "        self._classification_report = classification_report(y, y_pred)\n",
    "        self.model = svc\n",
    "        \n",
    "        if self.show_value:\n",
    "            print(\"Confusion Matrix:\")\n",
    "            print(self._confusion_matrix)\n",
    "            print(\"Classification Report:\")\n",
    "            print(self._classification_report)\n",
    "\n",
    "    def predict(self) -> list:\n",
    "        \"\"\"\n",
    "        Predicts the class of the input\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise Exception(\"Model not found. Run fit() first.\")\n",
    "        \n",
    "        df = build_dataframe(source_dir=self._stream_csv_dir)\n",
    "        X = df.drop(\n",
    "            [\"filename\", \"class\", \"class_num\"], axis=1\n",
    "        )\n",
    "                \n",
    "        y_pred = self.model.predict(X)\n",
    "        \n",
    "        classnum_pred = y_pred[0]\n",
    "        classname_pred = num_to_class_dict[classnum_pred]\n",
    "        \n",
    "        print(f\"Predicted class: {classname_pred}\")\n",
    "        print(f\"Predicted class number: {classnum_pred}\")\n",
    "            \n",
    "\n",
    "class StreamEmbedder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        stream_video: str,\n",
    "        stream_image_out_dir: str,\n",
    "        stream_csv_out_dir: str,\n",
    "        output_video_path: str,\n",
    "    ):\n",
    "        self.stream_video = stream_video\n",
    "        self.stream_image_out_dir = stream_image_out_dir\n",
    "        self.stream_csv_out_dir = stream_csv_out_dir\n",
    "        self.output_video_path = output_video_path\n",
    "        self.bootstrap_helper = BootstrapHelper(\n",
    "            images_out_folder=stream_image_out_dir,\n",
    "            csvs_out_folder=stream_csv_out_dir,\n",
    "        )\n",
    "\n",
    "    def _get_frame_from_stream(self):\n",
    "        \"\"\"\n",
    "        Returns the next frame from the video stream\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(self.stream_video)\n",
    "        _, frame = cap.read()\n",
    "        cap.release()\n",
    "        return frame\n",
    "\n",
    "    def _save_frame_to_directory(self, frame, directory):\n",
    "        \"\"\"\n",
    "        Saves the given frame to the given directory\n",
    "        \"\"\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        filename = f\"{datetime.now().strftime('%Y%m%d-%H%M%S-%f')}.jpg\"\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        cv2.imwrite(filepath, frame)\n",
    "        \n",
    "    def _remove_image_from_in_out_dir(self) -> None:\n",
    "        target_dir_to_clean_in = os.path.join(self.stream_image_out_dir, \"stream\")\n",
    "        self._clean_directory(directory=target_dir_to_clean_in)\n",
    "\n",
    "    def generate_embbedings(self) -> None:\n",
    "        \"\"\"\n",
    "        Returns the embeddings from the video stream\n",
    "        \"\"\"\n",
    "\n",
    "        self.bootstrap_helper.bootstrap()\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(self.output_video_path, fourcc, 20.0, (640, 480))\n",
    "        while True:\n",
    "            frame = self._get_frame_from_stream()\n",
    "            self._save_frame_to_directory(frame, os.path.join(self.stream_image_out_dir, \"stream\"))\n",
    "            out.write(frame)\n",
    "            cv2.imshow('frame', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                self._save_frame_to_directory(frame, os.path.join(self.stream_image_out_dir, \"stream\"))\n",
    "                break\n",
    "        out.release()\n",
    "        self._remove_image_from_in_out_dir()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from pose_classification import classify_posture\n",
    "from pose_estimation import PoseEstimator\n",
    "\n",
    "def save_video(frame, out):\n",
    "    out.write(frame)\n",
    "\n",
    "def display_posture(frame, pose, posture_pred):\n",
    "    display = pose.draw_landmarks(frame, posture_pred)\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Yoga posture classifier')\n",
    "    parser.add_argument('-i', '--input', type=str, required=True, help='Path to input video file or \"camera\" for webcam')\n",
    "    parser.add_argument('--display', action='store_true', help='Display the video stream with the predicted posture')\n",
    "    parser.add_argument('--output_path', type=str, help='Path to output video file')\n",
    "    parser.add_argument('--save_model_path', type=str, help='Path to save the trained SVM model')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.input == \"camera\":\n",
    "        cap = cv2.VideoCapture(0)\n",
    "    else:\n",
    "        cap = cv2.VideoCapture(args.input)\n",
    "    if args.output_path is not None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(args.output_path, fourcc, 20.0, (640, 480))\n",
    "    start_time = time.time()\n",
    "    posture_timer = 0\n",
    "    posture_count = 0\n",
    "    pose = PoseEstimator()\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        posture_pred = classify_posture(frame, pose, svm)\n",
    "        if posture_pred == \"maintained\":\n",
    "            posture_timer = time.time() - start_time\n",
    "            if posture_timer >= 30:\n",
    "                # Flash the image\n",
    "                cv2.imshow('frame', frame)\n",
    "                cv2.waitKey(1000)\n",
    "                cv2.imshow('frame', np.zeros_like(frame))\n",
    "                cv2.waitKey(1000)\n",
    "                # Display progress line\n",
    "                posture_count += 1\n",
    "                progress = int((posture_timer / 30) * 100)\n",
    "                cv2.putText(frame, f\"Progress: {progress}%\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        else:\n",
    "            start_time = time.time()\n",
    "            posture_timer = 0\n",
    "        if args.display:\n",
    "            display = display_posture(frame, pose, posture_pred)\n",
    "            if not display:\n",
    "                break\n",
    "        if args.output_path is not None:\n",
    "            save_video(frame, out)\n",
    "    cap.release()\n",
    "    if args.output_path is not None:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
