{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import argparse\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "def load_yoga_postures(posture_dir):\n",
    "    yoga_postures = []\n",
    "    yoga_posture_labels = []\n",
    "\n",
    "    for posture_name in os.listdir(posture_dir):\n",
    "        if posture_name.startswith(\".\"):\n",
    "            continue\n",
    "        posture_folder = os.path.join(posture_dir, posture_name)\n",
    "        for image_name in os.listdir(posture_folder):\n",
    "            if image_name.startswith(\".\"):\n",
    "                continue\n",
    "            image_path = os.path.join(posture_folder, image_name)\n",
    "            if image_name.endswith(\".jpg\") or image_name.endswith(\".png\"):\n",
    "                image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                image = cv2.resize(image, (64, 64))\n",
    "                yoga_postures.append(image)\n",
    "                yoga_posture_labels.append(posture_name)\n",
    "\n",
    "    return yoga_postures, yoga_posture_labels\n",
    "\n",
    "def train_svm_model(X_train, yoga_posture_labels):\n",
    "    svm = SVC(kernel=\"linear\", C=1.0, random_state=42)\n",
    "    svm.fit(X_train, yoga_posture_labels)\n",
    "    return svm\n",
    "\n",
    "def classify_posture(frame, pose, svm, threshold=0.75):\n",
    "        # Convert the frame to grayscale\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # Detect the pose landmarks using Mediapipe\n",
    "        results = pose.process(frame_gray)\n",
    "        if results.pose_landmarks is not None:\n",
    "            # Extract the pose landmarks and calculate the HOG features\n",
    "            landmarks = np.array([[lmk.x, lmk.y] for lmk in results.pose_landmarks.landmark])\n",
    "            landmarks = landmarks.flatten()\n",
    "            # Predict the posture using the SVM model\n",
    "            scores = svm.decision_function([landmarks])\n",
    "            if np.max(scores) >= threshold:\n",
    "                posture_pred = svm.predict([landmarks])[0]\n",
    "            else:\n",
    "                posture_pred = None\n",
    "        else:\n",
    "            posture_pred = None\n",
    "        return posture_pred\n",
    "\n",
    "def display_posture(frame, pose, posture_pred):\n",
    "    # Draw the pose landmarks and the predicted posture on the frame\n",
    "    mp_drawing.draw_landmarks(frame, pose.process(frame).pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    cv2.putText(frame, posture_pred, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def save_video(frame, out):\n",
    "    out.write(frame)\n",
    "\n",
    "def yoga_classifier(input_path, display=False, output_path=None, save_model_path=None):\n",
    "    # Load the yoga postures and their labels\n",
    "    posture_dir = \"/home/khaleb.dabakuyo@Digital-Grenoble.local/Documents/ACV/Panther_trainer2/assets/images/train\"\n",
    "    yoga_postures, yoga_posture_labels = load_yoga_postures(posture_dir)\n",
    "\n",
    "    if save_model_path is not None and os.path.exists(save_model_path):\n",
    "        # Load the saved SVM model\n",
    "        svm = joblib.load(save_model_path)\n",
    "    else:\n",
    "        # Extract features from the images using mediapipe\n",
    "        X_train = []\n",
    "        for posture in yoga_postures:\n",
    "            results = mp_pose.Pose().process(posture)\n",
    "            if results.pose_landmarks is not None:\n",
    "                landmarks = np.array([[lmk.x, lmk.y] for lmk in results.pose_landmarks.landmark])\n",
    "                landmarks = landmarks.flatten()\n",
    "                X_train.append(landmarks)\n",
    "        X_train = np.array(X_train)\n",
    "        # Train a SVM model on the extracted features\n",
    "        svm = train_svm_model(X_train, yoga_posture_labels)\n",
    "        if save_model_path is not None:\n",
    "            # Save the trained SVM model\n",
    "            joblib.dump(svm, save_model_path)\n",
    "\n",
    "    # Initialize Mediapipe pose detection\n",
    "    pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "    # Use the trained model to classify new images of the yoga postures\n",
    "    if input_path == \"camera\":\n",
    "        cap = cv2.VideoCapture(0)\n",
    "    else:\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "    if output_path is not None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, 20.0, (640, 480))\n",
    "    start_time = time.time()\n",
    "    posture_timer = 0\n",
    "    posture_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        posture_pred = classify_posture(frame, pose, svm)\n",
    "        if posture_pred == \"maintained\":\n",
    "            posture_timer = time.time() - start_time\n",
    "            if posture_timer >= 30:\n",
    "                # Flash the image\n",
    "                cv2.imshow('frame', frame)\n",
    "                cv2.waitKey(1000)\n",
    "                cv2.imshow('frame', np.zeros_like(frame))\n",
    "                cv2.waitKey(1000)\n",
    "                # Display progress line\n",
    "                posture_count += 1\n",
    "                progress = int((posture_timer / 30) * 100)\n",
    "                cv2.putText(frame, f\"Progress: {progress}%\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        else:\n",
    "            start_time = time.time()\n",
    "            posture_timer = 0\n",
    "        if display:\n",
    "            display = display_posture(frame, pose, posture_pred)\n",
    "            if not display:\n",
    "                break\n",
    "        if output_path is not None:\n",
    "            save_video(frame, out)\n",
    "    cap.release()\n",
    "    if output_path is not None:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n",
      "Premature end of JPEG file\n",
      "I0000 00:00:1700046578.734850   30009 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1700046578.795137   30872 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 525.125.06), renderer: NVIDIA GeForce GTX 1650/PCIe/SSE2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/khaleb.dabakuyo@Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m yoga_classifier(\u001b[39m'\u001b[39m\u001b[39mcamera\u001b[39m\u001b[39m'\u001b[39m, display\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, output_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, save_model_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrue\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/khaleb.dabakuyo@Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb Cell 2\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W1sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m X_train \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W1sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39mfor\u001b[39;00m posture \u001b[39min\u001b[39;00m yoga_postures:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W1sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     results \u001b[39m=\u001b[39m mp_pose\u001b[39m.\u001b[39mPose()\u001b[39m.\u001b[39mprocess(posture)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W1sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/khaleb.dabakuyo%40Digital-Grenoble.local/Documents/ACV/Panther_trainer2/yc_notebook.ipynb#W1sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m         landmarks \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[lmk\u001b[39m.\u001b[39mx, lmk\u001b[39m.\u001b[39my] \u001b[39mfor\u001b[39;00m lmk \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks\u001b[39m.\u001b[39mlandmark])\n",
      "File \u001b[0;32m~/anaconda3/envs/acv/lib/python3.11/site-packages/mediapipe/python/solutions/pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, image: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NamedTuple:\n\u001b[1;32m    165\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mprocess(input_data\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m: image})\n\u001b[1;32m    186\u001b[0m   \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks:  \u001b[39m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[39mfor\u001b[39;00m landmark \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks\u001b[39m.\u001b[39mlandmark:  \u001b[39m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/acv/lib/python3.11/site-packages/mediapipe/python/solution_base.py:360\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    355\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSolutionBase can only process non-audio and non-proto-list data. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    356\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_stream_type_info[stream_name]\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    357\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtype is not supported yet.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    358\u001b[0m \u001b[39melif\u001b[39;00m (input_stream_type \u001b[39m==\u001b[39m PacketDataType\u001b[39m.\u001b[39mIMAGE_FRAME \u001b[39mor\u001b[39;00m\n\u001b[1;32m    359\u001b[0m       input_stream_type \u001b[39m==\u001b[39m PacketDataType\u001b[39m.\u001b[39mIMAGE):\n\u001b[0;32m--> 360\u001b[0m   \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m] \u001b[39m!=\u001b[39m RGB_CHANNELS:\n\u001b[1;32m    361\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInput image must contain three channel rgb data.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    362\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    363\u001b[0m       stream\u001b[39m=\u001b[39mstream_name,\n\u001b[1;32m    364\u001b[0m       packet\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    365\u001b[0m                                data)\u001b[39m.\u001b[39mat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simulated_timestamp))\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "yoga_classifier('camera', display=True, output_path=None, save_model_path='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--display-landmarks]\n",
      "                             [--output-path OUTPUT_PATH]\n",
      "                             [--save-model-path SAVE_MODEL_PATH]\n",
      "                             [--display-progress]\n",
      "                             input_path\n",
      "ipykernel_launcher.py: error: the following arguments are required: input_path\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khaleb.dabakuyo@Digital-Grenoble.local/anaconda3/envs/acv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import argparse\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "def load_yoga_postures(posture_dir):\n",
    "    yoga_postures = []\n",
    "    yoga_posture_labels = []\n",
    "\n",
    "    for posture_name in os.listdir(posture_dir):\n",
    "        if posture_name.startswith(\".\"):\n",
    "            continue\n",
    "        posture_folder = os.path.join(posture_dir, posture_name)\n",
    "        for image_name in os.listdir(posture_folder):\n",
    "            if image_name.startswith(\".\"):\n",
    "                continue\n",
    "            image_path = os.path.join(posture_folder, image_name)\n",
    "            if image_name.endswith(\".jpg\") or image_name.endswith(\".png\"):\n",
    "                image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                image = cv2.resize(image, (64, 64))\n",
    "                yoga_postures.append(image)\n",
    "                yoga_posture_labels.append(posture_name)\n",
    "\n",
    "    return yoga_postures, yoga_posture_labels\n",
    "\n",
    "def train_svm_model(X_train, yoga_posture_labels):\n",
    "    svm = SVC(kernel=\"linear\", C=1.0, random_state=42)\n",
    "    svm.fit(X_train, yoga_posture_labels)\n",
    "    return svm\n",
    "\n",
    "def classify_posture(frame, pose, svm, threshold=0.75):\n",
    "        # Convert the frame to grayscale\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # Detect the pose landmarks using Mediapipe\n",
    "        results = pose.process(frame_gray)\n",
    "        if results.pose_landmarks is not None:\n",
    "            # Extract the pose landmarks and calculate the HOG features\n",
    "            landmarks = np.array([[lmk.x, lmk.y] for lmk in results.pose_landmarks.landmark])\n",
    "            landmarks = landmarks.flatten()\n",
    "            # Predict the posture using the SVM model\n",
    "            scores = svm.decision_function([landmarks])\n",
    "            if np.max(scores) >= threshold:\n",
    "                posture_pred = svm.predict([landmarks])[0]\n",
    "                confidence = np.max(scores)\n",
    "            else:\n",
    "                posture_pred = None\n",
    "                confidence = None\n",
    "        else:\n",
    "            posture_pred = None\n",
    "            confidence = None\n",
    "        return posture_pred, confidence\n",
    "\n",
    "def display_posture(frame, pose, posture_pred, posture_timer, confidence, display_landmarks=False, display_progress=False):\n",
    "    # Draw the pose landmarks and the predicted posture on the frame\n",
    "    if display_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, pose.process(frame).pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    if posture_pred is not None:\n",
    "        cv2.putText(frame, posture_pred, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        if posture_timer >= 30 and display_progress:\n",
    "            # Display progress bar\n",
    "            progress = int((posture_timer / 30) * 100)\n",
    "            cv2.rectangle(frame, (50, 80), (50 + progress, 100), (0, 255, 0), -1)\n",
    "            cv2.putText(frame, f\"Progress: {progress}%\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            if progress == 100:\n",
    "                # Flash the image\n",
    "                cv2.imshow('frame', frame)\n",
    "                cv2.waitKey(1000)\n",
    "                cv2.imshow('frame', np.zeros_like(frame))\n",
    "                cv2.waitKey(1000)\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def save_video(frame, out):\n",
    "    out.write(frame)\n",
    "\n",
    "def classify_yoga_postures(input_path, display_landmarks=True, output_path=None, save_model_path=None, display_progress=True):\n",
    "    # Load the yoga postures and their labels\n",
    "    posture_dir = \"/home/khaleb.dabakuyo@Digital-Grenoble.local/Documents/ACV/Panther_trainer2/assets/images/train\"\n",
    "    yoga_postures, yoga_posture_labels = load_yoga_postures(posture_dir)\n",
    "\n",
    "    if save_model_path is not None and os.path.exists(save_model_path):\n",
    "        # Load the saved SVM model\n",
    "        svm = joblib.load(save_model_path)\n",
    "    else:\n",
    "        # Extract features from the images using mediapipe\n",
    "        X_train = []\n",
    "        for posture in yoga_postures:\n",
    "            results = mp_pose.Pose().process(posture)\n",
    "            if results.pose_landmarks is not None:\n",
    "                landmarks = np.array([[lmk.x, lmk.y] for lmk in results.pose_landmarks.landmark])\n",
    "                landmarks = landmarks.flatten()\n",
    "                X_train.append(landmarks)\n",
    "        X_train = np.array(X_train)\n",
    "        # Train a SVM model on the extracted features\n",
    "        svm = train_svm_model(X_train, yoga_posture_labels)\n",
    "        if save_model_path is not None:\n",
    "            # Save the trained SVM model\n",
    "            joblib.dump(svm, save_model_path)\n",
    "\n",
    "    # Initialize Mediapipe pose detection\n",
    "    pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "    # Use the trained model to classify new images of the yoga postures\n",
    "    if input_path == \"camera\":\n",
    "        cap = cv2.VideoCapture(0)\n",
    "    else:\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "    if output_path is not None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, 20.0, (640, 480))\n",
    "    start_time = time.time()\n",
    "    posture_timer = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        posture_pred, confidence = classify_posture(frame, pose, svm)\n",
    "        if posture_pred == \"maintained\":\n",
    "            posture_timer = time.time() - start_time\n",
    "            if posture_timer >= 30 and display_progress:\n",
    "                # Display progress bar\n",
    "                display_posture(frame, pose, posture_pred, posture_timer, confidence, display_landmarks, display_progress)\n",
    "        else:\n",
    "            start_time = time.time()\n",
    "            posture_timer = 0\n",
    "        if display_landmarks:\n",
    "            display = display_posture(frame, pose, posture_pred, posture_timer, confidence, display_landmarks, display_progress)\n",
    "            if not display:\n",
    "                break\n",
    "        if output_path is not None:\n",
    "            save_video(frame, out)\n",
    "    cap.release()\n",
    "    if output_path is not None:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Classify yoga postures from webcam or video file.')\n",
    "    parser.add_argument('input_path', type=str, help='Path to input file or \"camera\" for webcam')\n",
    "    parser.add_argument('--display-landmarks', action='store_true', help='Display landmarks on output video')\n",
    "    parser.add_argument('--output-path', type=str, help='Path to output file')\n",
    "    parser.add_argument('--save-model-path', type=str, help='Path to save trained model')\n",
    "    parser.add_argument('--display-progress', action='store_true', help='Display progress bar and flash')\n",
    "    args = parser.parse_args()\n",
    "    classify_yoga_postures(args.input_path, args.display_landmarks, args.output_path, args.save_model_path, args.display_progress)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
